{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBI4-qEDI_Gu",
        "outputId": "af8aec72-d667-41b0-c81d-2f66c611354a"
      },
      "outputs": [],
      "source": [
        "!gdown --id dataset_id --output train.csv\n",
        "!gdown --id  dataset_id test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbCAUSIsIqpD"
      },
      "outputs": [],
      "source": [
        "%pip install -U scikit-learn\n",
        "%pip install xgboost\n",
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcppWjJWJ222"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import wandb\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "from scipy.stats import uniform, randint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzgfYBil7Ejw"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-dQaHeOJWBw"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size' : 10000,\n",
        "    'batch_number' : 500,\n",
        "    'train_file' : \"train.csv\",\n",
        "    'test_file' : \"test.csv\",\n",
        "    'feature_selected' : [4, 5, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23],\n",
        "    'all_feature' : True,\n",
        "    'all_data' : False\n",
        "}\n",
        "\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vNJ88cP670s"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiQC36kTLU2J"
      },
      "outputs": [],
      "source": [
        "def select_random_numbers(lower_limit, upper_limit, n):\n",
        "\n",
        "  selected_numbers = random.sample(range(lower_limit, upper_limit + 1), n)\n",
        "\n",
        "  sorted_result = sorted(selected_numbers)\n",
        "\n",
        "  return sorted_result\n",
        "\n",
        "\n",
        "def feature_select(x_train, x_test):\n",
        "    if(config['all_feature']):\n",
        "\n",
        "      return x_train, x_test\n",
        "    else:\n",
        "      select_idx = config['all_feature']\n",
        "\n",
        "      return x_train[:, select_idx], x_test[:, select_idx]\n",
        "\n",
        "\n",
        "\n",
        "def create_zip_archive(source_folder, zip_filename):\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        for root, dirs, files in os.walk(source_folder):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, source_folder)\n",
        "                zipf.write(file_path, arcname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw7zlw3p6_6s"
      },
      "source": [
        "### Preprocessing Fucntions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JcAF2ElUqVw"
      },
      "outputs": [],
      "source": [
        "def preprocess_training_data(data, imputer = None):\n",
        "\n",
        "  print(\"Dropping attributes...\")\n",
        "\n",
        "  data = data.drop([\"txkey\"], axis=1)\n",
        "\n",
        "  # Encode data\n",
        "  print(\"Encoding attributes...\")\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  id_type_columns = [\"chid\", \"cano\", \"mchno\", \"acqic\"]\n",
        "\n",
        "  for column in id_type_columns:\n",
        "      series = pd.Series(data[column])\n",
        "      label_encoder.fit(series)\n",
        "      data[column] = label_encoder.transform(data[column])\n",
        "\n",
        "  numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "  # Deal with NaN\n",
        "  data[\"stscd\"] = data[\"stscd\"].fillna(0)\n",
        "  if imputer == None:\n",
        "    data = data.dropna(axis = 0, how = \"any\")\n",
        "  else:\n",
        "    data[numeric_features] = imputer.fit_transform(data[numeric_features])\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def preprocess_testing_data(data, imputer = None):\n",
        "  test_data_keys = data[\"txkey\"]\n",
        "  data = data.drop([\"txkey\"], axis = 1)\n",
        "\n",
        "\n",
        "  # encode data\n",
        "  print(\"Encoding attributes...\")\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  id_type_columns = [\"chid\", \"cano\", \"mchno\", \"acqic\"]\n",
        "\n",
        "  for column in id_type_columns:\n",
        "      series = pd.Series(data[column])\n",
        "      label_encoder.fit(series)\n",
        "      data[column] = label_encoder.transform(data[column])\n",
        "\n",
        "  # Deal with NaN\n",
        "  data[\"stscd\"] = data[\"stscd\"].fillna(0)\n",
        "  if imputer != None:\n",
        "    data = pd.DataFrame(imputer.fit_transform(data))\n",
        "\n",
        "  return data, test_data_keys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3jjktdD6sUJ"
      },
      "source": [
        "### Plot the distribution of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmoFLOTcpWOl"
      },
      "outputs": [],
      "source": [
        "def plot_data_distribution(data_before, data_after, columns, sample_size=None):\n",
        "    if sample_size is not None:\n",
        "        data_before = data_before.sample(n=sample_size, random_state=42)\n",
        "        data_after = data_after.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    print(data_before.shape)\n",
        "    print(data_after.shape)\n",
        "\n",
        "    common_columns = set(data_before.columns).intersection(set(data_after.columns))\n",
        "    data_before = data_before[common_columns]\n",
        "    data_after = data_after[common_columns]\n",
        "\n",
        "    figure_type_number = len(common_columns)\n",
        "\n",
        "    save_dir = \"src\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for i, column in enumerate(common_columns):\n",
        "        print(\"Column Processing: \" + column)\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "        fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(data_before[column]):\n",
        "            sns.kdeplot(data_before[column], ax=axes[0])\n",
        "            axes[0].set_title(f'{column} - before')\n",
        "\n",
        "            sns.kdeplot(data_after[column], ax=axes[1])\n",
        "            axes[1].set_title(f'{column} - after')\n",
        "\n",
        "        else:\n",
        "            axes[0].set_title(f'{column} - before')\n",
        "\n",
        "            sns.kdeplot(data_after[column], ax=axes[1])\n",
        "            axes[1].set_title(f'{column} - after')\n",
        "\n",
        "        print(\"{} in {} diagrams is printed.\".format(i + 1, figure_type_number))\n",
        "\n",
        "        save_path = os.path.join(save_dir, f'{column}_comparison.png')\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULgXzda6oCB"
      },
      "source": [
        "### Training Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0svNyAkJvzo"
      },
      "outputs": [],
      "source": [
        "batch_size = config['batch_size']\n",
        "train_file = config['train_file']\n",
        "\n",
        "training_data = pd.read_csv(train_file, nrows=1)\n",
        "\n",
        "original_columns = training_data.columns.tolist()\n",
        "\n",
        "training_data = pd.DataFrame()\n",
        "\n",
        "# imputer = IterativeImputer(max_iter=8, random_state=0)\n",
        "imputer = None\n",
        "\n",
        "training_data_before = pd.DataFrame()\n",
        "training_data_after = pd.DataFrame()\n",
        "selected_chunk_idx = []\n",
        "all_data = config['all_data']\n",
        "batch_number = config['batch_number']\n",
        "i, j = 0, 0\n",
        "\n",
        "if not(all_data):\n",
        "  selected_chunk_idx = select_random_numbers(0, 868, config['batch_number'])\n",
        "\n",
        "\n",
        "for chunk in pd.read_csv(train_file, chunksize=batch_size, skiprows=batch_size):\n",
        "\n",
        "  if not(all_data):\n",
        "    if j >= batch_number:\n",
        "      break\n",
        "    elif selected_chunk_idx[j] != i:\n",
        "      i += 1\n",
        "      continue\n",
        "    else:\n",
        "      j += 1\n",
        "\n",
        "\n",
        "  print(\"Start Loading batch no.{} ...\".format(i))\n",
        "  i += 1\n",
        "\n",
        "  loaded = i if all_data == True else j\n",
        "  print(\"Already Load {} batches !!\".format(loaded))\n",
        "\n",
        "  chunk.columns = original_columns\n",
        "  chunk_before = chunk.copy()\n",
        "  chunk_after = preprocess_training_data(chunk_before, imputer)\n",
        "\n",
        "\n",
        "  training_data_before = pd.concat([training_data_before, chunk_before], ignore_index=True)\n",
        "  training_data_after = pd.concat([training_data_after, chunk_after], ignore_index=True)\n",
        "\n",
        "  training_data = pd.concat([training_data, chunk_after], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyCjtpv8D3nd",
        "outputId": "28b31035-667e-42a4-c2fa-c4a3254dc412"
      },
      "outputs": [],
      "source": [
        "print(training_data['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9yOQ2D70C6"
      },
      "source": [
        "### Testing Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE908pHg7mNa",
        "outputId": "2f5008ae-7de1-4c4c-addc-3e2a10a2b950"
      },
      "outputs": [],
      "source": [
        "test_file  = config['test_file']\n",
        "testing_data = pd.read_csv(test_file)\n",
        "\n",
        "imputer = IterativeImputer(max_iter=8, random_state=0)\n",
        "\n",
        "testing_data, test_data_keys = preprocess_testing_data(testing_data, imputer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLDxeyw39qYF"
      },
      "outputs": [],
      "source": [
        "print(testing_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAk4D7wN8O3e"
      },
      "source": [
        "### Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4iWSOe1wiRi"
      },
      "outputs": [],
      "source": [
        "plot_data_distribution(training_data_before, training_data_after, training_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56sbGyrT3xYX",
        "outputId": "8f0439af-689a-4dd4-ff5b-40a60a5bd758"
      },
      "outputs": [],
      "source": [
        "source_directory = \"src\"\n",
        "zip_file_name = \"src.zip\"\n",
        "\n",
        "create_zip_archive(source_directory, zip_file_name)\n",
        "print(f\"{zip_file_name} created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hC2NHu48TKw"
      },
      "source": [
        "### Final Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEAxIiQBTS8L"
      },
      "outputs": [],
      "source": [
        "# seperate training data label\n",
        "training_data_labels = training_data[\"label\"]\n",
        "training_data = training_data.drop([\"label\"], axis = 1)\n",
        "\n",
        "# transform data to array\n",
        "print(\"Transforming data frames to arrays...\")\n",
        "training_data_2Darray = training_data.values\n",
        "training_data_label_array = training_data_labels.values\n",
        "test_data_2Darray = testing_data.values\n",
        "test_data_key_array = test_data_keys.values\n",
        "training_data_2Darray, test_data_2Darray = feature_select(training_data_2Darray, test_data_2Darray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGw0IFpm8ZZm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwZDvYCScA9t"
      },
      "outputs": [],
      "source": [
        "param_dist = {\n",
        "    'learning_rate': uniform(0, 0.5),\n",
        "    'max_depth': randint(3, 12),\n",
        "    'n_estimators': randint(200, 400),\n",
        "    'subsample': uniform(0.2, 0.5),\n",
        "    'colsample_bytree': uniform(0.2, 0.8),\n",
        "    'gamma': uniform(0.0, 10.0)\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(XGBClassifier(), param_distributions=param_dist, n_iter=30, cv=5, random_state=42, scoring='f1')\n",
        "random_search.fit(training_data_2Darray, training_data_label_array)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "print(best_params)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "feature_importance = best_model.feature_importances_\n",
        "\n",
        "predicted_data = best_model.predict(test_data_2Darray)\n",
        "\n",
        "prediction_pair_data = [[\"txkey\", \"pred\"]]\n",
        "for i in range(len(test_data_key_array)):\n",
        "    prediction_pair_data.append([str(test_data_key_array[i]), str(int(predicted_data[i]))])\n",
        "\n",
        "pd.DataFrame(prediction_pair_data).to_csv(\"result.csv\", header=None, index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLISnYRBwkGb"
      },
      "outputs": [],
      "source": [
        "feature_names = training_data.columns.tolist()\n",
        "\n",
        "print(feature_importance)\n",
        "\n",
        "selected_idx = config['feature_selected']\n",
        "if(config['select_all'] == False):\n",
        "  feature_names = [feature_names[i] for i in selected_idx]\n",
        "\n",
        "feature_df = pd.DataFrame({'FeatureName': feature_names, 'Importance': feature_importance})\n",
        "\n",
        "csv_file_path = 'feature_importance.csv'\n",
        "feature_df.to_csv(csv_file_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
