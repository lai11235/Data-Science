{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlJEy2uAuTe6"
      },
      "source": [
        "### Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrqdNus7uTfA",
        "outputId": "612e8811-add2-4196-b98c-ca1507e1afd9"
      },
      "outputs": [],
      "source": [
        "!gdown --id dataset_id --output train.csv\n",
        "!gdown --id dataset_id --output test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phd2WY8KuTfB"
      },
      "source": [
        "### Import package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvMZIrNBuTfB",
        "outputId": "87c519c3-6529-4ba7-e11a-a798cfb33072"
      },
      "outputs": [],
      "source": [
        "%pip install -U scikit-learn\n",
        "%pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sp93v0zGuTfC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import csv\n",
        "\n",
        "import calendar\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46HbvlEhuTfC"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UmFiUeuEuTfC"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'all_feature' : False,\n",
        "    'train_data' : \"./train.csv\",\n",
        "    'test_data' : \"./test.csv\",\n",
        "    'feature_selected' : [0, 2, 3, 4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8pjsjfHouTfD"
      },
      "outputs": [],
      "source": [
        "def feature_select(x_train, x_test):\n",
        "  if(config['all_feature']):\n",
        "    return x_train, x_test\n",
        "  else:\n",
        "    select_idx = config['feature_selected']\n",
        "    return x_train[:, select_idx], x_test[:, select_idx]\n",
        "\n",
        "def date_info_processing(data):\n",
        "  for i in range(len(data)):\n",
        "    current_day = datetime.strptime(data[i][0], '%Y-%m-%d')\n",
        "    current_day_pos = current_day.timetuple().tm_yday\n",
        "    day_count_of_year = 366 if calendar.isleap(current_day.year) else 365\n",
        "    data[i][0] = float(current_day_pos / day_count_of_year)\n",
        "\n",
        "  return data\n",
        "\n",
        "def drop_features(data, features_to_drop):\n",
        "  data.drop(columns=features_to_drop, axis = 1, inplace = True)\n",
        "\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcvxWHeKOk9q"
      },
      "source": [
        "### Data Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z26so3w5GYmW"
      },
      "outputs": [],
      "source": [
        "def array_threshold(tar: np.ndarray):\n",
        "  mask = abs(tar) < 0.00001\n",
        "  tar[mask] = 0\n",
        "\n",
        "  return tar\n",
        "\n",
        "def date_processing(data: pd.DataFrame):\n",
        "  tar_pos = data.columns.get_loc('Attribute1')\n",
        "\n",
        "  data['Attribute1'] = pd.to_datetime(data['Attribute1'])\n",
        "\n",
        "  month_df = data['Attribute1'].dt.month\n",
        "  month = data['Attribute1'].dt.month.values.tolist()\n",
        "\n",
        "  data.drop('Attribute1', axis=1, inplace=True)\n",
        "  data_len = len(data)\n",
        "  zero_list = [0] * data_len\n",
        "  data.insert(loc=tar_pos + 0, column='Month', value=zero_list)\n",
        "  data.insert(loc=tar_pos + 1, column='MonthSin', value=zero_list)\n",
        "  data.insert(loc=tar_pos + 2, column='MonthCos', value=zero_list)\n",
        "\n",
        "  month_sin_arr = np.sin(2 * np.pi * month_df / 12)\n",
        "  month_cos_arr = np.cos(2 * np.pi * month_df / 12)\n",
        "\n",
        "  month_sin_arr = array_threshold(month_sin_arr)\n",
        "  month_cos_arr = array_threshold(month_cos_arr)\n",
        "\n",
        "  data['Month'] = month\n",
        "  data['MonthSin'] = month_sin_arr\n",
        "  data['MonthCos'] = month_cos_arr\n",
        "\n",
        "  return data\n",
        "\n",
        "def get_season(month):\n",
        "  if 3 <= month <= 5:\n",
        "      return 0\n",
        "  elif 6 <= month <= 8:\n",
        "      return 1\n",
        "  elif 9 <= month <= 11:\n",
        "      return 2\n",
        "  else:\n",
        "      return 3\n",
        "\n",
        "def season_mapping(data: pd.DataFrame):\n",
        "  tar_pos = data.columns.get_loc('Attribute20')\n",
        "\n",
        "  month_df = data['Month']\n",
        "  season = data['Month'].apply(get_season)\n",
        "  data.insert(loc=tar_pos + 1, column='Season', value=season)\n",
        "\n",
        "  return data\n",
        "\n",
        "def binary_processing(data: pd.DataFrame, is_test):\n",
        "  data['Attribute20'] = data['Attribute20'].apply(lambda x: 1 if x == 'Yes' else 0 if x == 'No' else x)\n",
        "\n",
        "  if not is_test:\n",
        "      data['Attribute21'] = data['Attribute21'].apply(lambda x: 1 if x == 'Yes' else 0 if x == 'No' else x)\n",
        "\n",
        "  return data\n",
        "\n",
        "def direction_processing(data: pd.DataFrame):\n",
        "  # Mapping every direction Info a vector info the length of the vector is 1 and the vector can represent the angle info\n",
        "  degrees = {\"N\": 0, \"NNE\": 0, \"NE\": 0, \"ENE\": 0, \"E\": 0, \"ESE\": 0, \"SE\": 0, \"SSE\": 0,\n",
        "          \"S\": 0, \"SSW\": 0, \"SW\": 0, \"WSW\": 0, \"W\": 0, \"WNW\": 0, \"NW\": 0, \"NNW\": 0}\n",
        "  dirs = {\"N\": [], \"NNE\": [], \"NE\": [], \"ENE\": [], \"E\": [], \"ESE\": [], \"SE\": [], \"SSE\": [],\n",
        "          \"S\": [], \"SSW\": [], \"SW\": [], \"WSW\": [], \"W\": [], \"WNW\": [], \"NW\": [], \"NNW\": []}\n",
        "  degree = 0\n",
        "  step = 22.5\n",
        "  for dir in dirs:\n",
        "    x_val = -math.cos(math.radians(degree))\n",
        "    y_val = -math.sin(math.radians(degree))\n",
        "\n",
        "    x_val = x_val if abs(x_val) > 0.2 else 0\n",
        "    y_val = y_val if abs(y_val) > 0.2 else 0\n",
        "    dirs[dir].append(x_val)\n",
        "    dirs[dir].append(y_val)\n",
        "    degrees[dir] = degree\n",
        "    degree += step\n",
        "\n",
        "  # Store the mapping result in the list\n",
        "  new_x1_feature = []\n",
        "  new_y1_feature = []\n",
        "  new_x2_feature = []\n",
        "  new_y2_feature = []\n",
        "  new_diff_feature = []\n",
        "\n",
        "  for i in range(len(data['Attribute8'])):\n",
        "    dir1 = data['Attribute8'][i]\n",
        "    if (dir1 is not np.NAN):\n",
        "        new_x1_feature.append(dirs[dir1][0])\n",
        "        new_y1_feature.append(dirs[dir1][1])\n",
        "    else:\n",
        "        new_x1_feature.append(np.NAN)\n",
        "        new_y1_feature.append(np.NAN)\n",
        "    dir2 = data['Attribute9'][i]\n",
        "    if (dir2 is not np.NAN):\n",
        "        new_x2_feature.append(dirs[dir2][0])\n",
        "        new_y2_feature.append(dirs[dir2][1])\n",
        "    else:\n",
        "        new_x2_feature.append(np.NAN)\n",
        "        new_y2_feature.append(np.NAN)\n",
        "\n",
        "  tar_pos = data.columns.get_loc('Attribute8')\n",
        "  data.drop(['Attribute8', 'Attribute9'], axis=1, inplace=True)\n",
        "  data.insert(loc=tar_pos, column='Attribute8_x', value=new_x1_feature)\n",
        "  data.insert(loc=tar_pos + 1, column='Attribute8_y', value=new_y1_feature)\n",
        "  data.insert(loc=tar_pos + 2, column='Attribute9_x', value=new_x2_feature)\n",
        "  data.insert(loc=tar_pos + 3, column='Attribute9_y', value=new_y2_feature)\n",
        "\n",
        "  return data\n",
        "\n",
        "def relative_humidity_to_absolute_humidity(relative_humidity, temperature, atmospheric_pressure):\n",
        "\n",
        "  saturation_vapor_pressure = 6.11 * 10**((7.5 * temperature) / (237.7 + temperature))\n",
        "  actual_vapor_pressure = (relative_humidity / 100) * saturation_vapor_pressure\n",
        "  absolute_humidity = (actual_vapor_pressure * 1000) / (0.622 * atmospheric_pressure)\n",
        "\n",
        "  return absolute_humidity\n",
        "\n",
        "def mositure_difference(data: pd.DataFrame):\n",
        "  mositure_am = data['Attribute12']\n",
        "  mositure_pm = data['Attribute13']\n",
        "  temperature_am = data['Attribute18']\n",
        "  temperature_pm = data['Attribute19']\n",
        "  pressure_am = data['Attribute14']\n",
        "  pressure_pm = data['Attribute15']\n",
        "\n",
        "  mositure_am = relative_humidity_to_absolute_humidity(mositure_am, temperature_am, pressure_am)\n",
        "  mositure_pm = relative_humidity_to_absolute_humidity(mositure_pm, temperature_pm, pressure_pm)\n",
        "\n",
        "  mositure_diff = mositure_pm - mositure_am;\n",
        "\n",
        "  return mositure_diff\n",
        "\n",
        "def wind_combination(data: pd.DataFrame):\n",
        "  data['Attribute8_x'] = data['Attribute10'] * data['Attribute8_x']\n",
        "  data['Attribute8_y'] = data['Attribute10'] * data['Attribute8_y']\n",
        "  data['Attribute9_x'] = data['Attribute11'] * data['Attribute9_x']\n",
        "  data['Attribute9_y'] = data['Attribute11'] * data['Attribute9_y']\n",
        "\n",
        "  return data\n",
        "\n",
        "def feature_difference(data: pd.DataFrame, interaction_features):\n",
        "  interation = pd.DataFrame()\n",
        "\n",
        "  for feature_pair in interaction_features:\n",
        "    feature1, feature2, new_feature_name = feature_pair\n",
        "\n",
        "    if(new_feature_name == 'Mositure_Difference'):\n",
        "      interation[new_feature_name] = mositure_difference(data)\n",
        "    else:\n",
        "      interation[new_feature_name] = data[feature1] - data[feature2]\n",
        "\n",
        "  tar_pos = data.columns.get_loc('Attribute20')\n",
        "\n",
        "  for new_feature_name in interation.columns:\n",
        "    tar_pos += 1\n",
        "    data.insert(loc=tar_pos, column=new_feature_name, value=interation[new_feature_name])\n",
        "\n",
        "  return data\n",
        "\n",
        "def feature_interaction(data: pd.DataFrame, interaction_features):\n",
        "  interation = pd.DataFrame()\n",
        "\n",
        "  for feature_pair in interaction_features:\n",
        "    feature1, feature2, new_feature_name = feature_pair\n",
        "    interation[new_feature_name] = data[feature1] * data[feature2]\n",
        "\n",
        "  tar_pos = data.columns.get_loc('Attribute20')\n",
        "\n",
        "  for new_feature_name in interation.columns:\n",
        "    tar_pos += 1\n",
        "    data.insert(loc=tar_pos, column=new_feature_name, value=interation[new_feature_name])\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehAzwp3TVKtp"
      },
      "source": [
        "### Feature Relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xGa2SFJ1R9S7"
      },
      "outputs": [],
      "source": [
        "feature_diff = [\n",
        "    ('Attribute4', 'Attribute3', 'Day_Temperature_Difference'),\n",
        "    ('Attribute19', 'Attribute18', 'Time_Temperature_Difference'),\n",
        "    ('Attribute11', 'Attribute10', 'Wind_Speed_Difference'),\n",
        "    ('Attribute13', 'Attribute12', 'Mositure_Difference'),\n",
        "    ('Attribute15', 'Attribute14', 'Pressure_Difference'),\n",
        "    ('Attribute17', 'Attribute16', 'Cloud_Difference')\n",
        "]\n",
        "\n",
        "feature_multi = [\n",
        "    ('Attribute13', 'Attribute19', 'Humidity_Temperature_Interact'),\n",
        "    ('Attribute5', 'Attribute11', 'Rainfall_Wind_Speed_Interact'),\n",
        "    ('Attribute5', 'Attribute9_x', 'Rainfall_Wind_Strength_X'),\n",
        "    ('Attribute6', 'Attribute7', 'Evaporation_Sun')\n",
        "]\n",
        "\n",
        "feature_drop = [\n",
        "    'Attribute10', 'Attribute11'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFpihnvaJjB0"
      },
      "source": [
        "### Load Data & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0oYaMa9Jh6w",
        "outputId": "b45e57ec-3151-449e-c9d9-7f8dbf0c05c4"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(config['train_data'])\n",
        "test_data = pd.read_csv(config['test_data'])\n",
        "\n",
        "train_data.dropna(subset=['Attribute8'])\n",
        "train_data.dropna(subset=['Attribute9'])\n",
        "\n",
        "# Date processing\n",
        "train_data = date_processing(train_data)\n",
        "test_data = date_processing(test_data)\n",
        "\n",
        "# Date processing\n",
        "train_data = season_mapping(train_data)\n",
        "test_data = season_mapping(test_data)\n",
        "\n",
        "# Binary processing\n",
        "train_data = binary_processing(train_data, False)\n",
        "test_data = binary_processing(test_data, True)\n",
        "\n",
        "# Direction processing\n",
        "train_data = direction_processing(train_data)\n",
        "test_data = direction_processing(test_data)\n",
        "\n",
        "# Interpolate the data\n",
        "train_data.interpolate(method='linear', inplace=True)\n",
        "train_data.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "# Multiply the wind vector by the wind speed\n",
        "train_data = wind_combination(train_data)\n",
        "test_data = wind_combination(test_data)\n",
        "\n",
        "# Create the new feature by calculating the difference between feature values\n",
        "train_data = feature_difference(train_data, feature_diff)\n",
        "test_data = feature_difference(test_data, feature_diff)\n",
        "\n",
        "# Create the new feature by multiplying the feature values.\n",
        "train_data = feature_interaction(train_data, feature_multi)\n",
        "test_data = feature_interaction(test_data, feature_multi)\n",
        "\n",
        "# Drop the feature did not need\n",
        "train_data = drop_features(train_data, feature_drop)\n",
        "test_data = drop_features(test_data, feature_drop)\n",
        "\n",
        "\n",
        "# Balance the data\n",
        "train_data = train_data.groupby('Attribute21')\n",
        "no = train_data.get_group(0)\n",
        "yes = train_data.get_group(1)\n",
        "no = no.sample(len(yes))\n",
        "train_data = pd.concat([yes, no], axis = 0)\n",
        "train_data = shuffle(train_data)\n",
        "\n",
        "\n",
        "# Fetch the feature name after series of operation\n",
        "feature_names = train_data.columns.tolist()\n",
        "\n",
        "# Truning Data to NumPy array\n",
        "train_data = train_data.values\n",
        "test_data = test_data.values\n",
        "\n",
        "\n",
        "# Spliting train_data into train_data and label\n",
        "x_train = train_data[:,:-1]\n",
        "y_train = train_data[:,-1]\n",
        "\n",
        "x_train, x_test = feature_select(x_train, test_data)\n",
        "\n",
        "x_train = x_train.astype(float)\n",
        "x_test = x_test.astype(float)\n",
        "\n",
        "y_train = y_train[:, np.newaxis]\n",
        "y_train = y_train.astype(int)\n",
        "\n",
        "y_train.squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnKrJ0UKWnHE"
      },
      "source": [
        "## Find the proper superparamer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovlg8XtuTnhx",
        "outputId": "aeae1757-8927-4530-b09b-b1721a57f643"
      },
      "outputs": [],
      "source": [
        "param_dist = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [250, 300, 350],\n",
        "    'max_depth': [5, 7, 9, 11],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.6, 0.65, 0.7],\n",
        "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8],\n",
        "    'gamma': [0.05, 0.1, 0.15],\n",
        "    'scale_pos_weight': [1, 2, 3]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(XGBClassifier(), param_distributions=param_dist, n_iter=200, cv=4, random_state=0, scoring='accuracy', verbose = 1, n_jobs = -1)\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "print(best_params)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "feature_importance = best_model.feature_importances_\n",
        "predictions = best_model.predict(x_test)\n",
        "\n",
        "# Store the reslut\n",
        "id = [str(i) + '.0' for i in range(len(predictions))]\n",
        "\n",
        "result = pd.DataFrame(predictions, index=id, columns=['ans'])\n",
        "result.index.name = 'id'\n",
        "\n",
        "result.to_csv('result.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEj4xA8HWus7"
      },
      "source": [
        "### Store the Feature Importance in to csv file to select proper features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "x9n_UrdyH-Zb"
      },
      "outputs": [],
      "source": [
        "# Stroe the feature importance to csv file\n",
        "data_feature_names = list(enumerate(feature_names))\n",
        "data_feature_names = data_feature_names[:-1]\n",
        "\n",
        "selected_idx = config['feature_selected']\n",
        "if(config['all_feature'] == False):\n",
        "  data_feature_names = [data_feature_names[i] for i in selected_idx]\n",
        "\n",
        "indices, names = zip(*data_feature_names)\n",
        "\n",
        "feature_df = pd.DataFrame({'Index': indices, 'FeatureName': names, 'Importance': feature_importance})\n",
        "\n",
        "feature_df = feature_df.sort_values(by='Importance')\n",
        "csv_file_path = 'feature_importance.csv'\n",
        "\n",
        "feature_df.to_csv(csv_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aufkKre9W6mP"
      },
      "source": [
        "### Utilize the discovered hyperparameters to enhance the predictive model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KRm37mdHMFC9"
      },
      "outputs": [],
      "source": [
        "params = {'subsample': 0.7, 'scale_pos_weight': 1, 'n_estimators': 250, 'min_child_weight': 5,\n",
        "      'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0.05, 'colsample_bytree': 0.7,  'random_state': 0}\n",
        "\n",
        "model = XGBClassifier(**params)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "id = [str(i) + '.0' for i in range(len(predictions))]\n",
        "\n",
        "result = pd.DataFrame(predictions, index=id, columns=['ans'])\n",
        "result.index.name = 'id'\n",
        "\n",
        "result.to_csv('result.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
