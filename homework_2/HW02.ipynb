{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOK1rTvxjKy2",
        "outputId": "36b1dd9c-03b8-4d6c-d902-ecf8c7d538c0"
      },
      "outputs": [],
      "source": [
        "!gdown --id dataset_id --output test.csv\n",
        "!gdown --id dataset_id --output train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCgUvw8lu8BF"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB_PPALKuikn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing.dummy import Pool\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQS8OVNLvjxv"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3pf59KWvjZS"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'all_feature' : False,\n",
        "    'train_data' : \"./train.csv\",\n",
        "    'test_data' : \"./test.csv\",\n",
        "    'threshold' : 0.6,\n",
        "    'feature_selected' : [0, 2, 3, 4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5msYtai7A68"
      },
      "outputs": [],
      "source": [
        "def pitch_processing(data: pd.DataFrame)->pd.DataFrame:\n",
        "  pitch_mapping = {'C': 0, 'C#': 1, 'D': 2, 'D#': 3, 'E': 4, 'F': 5, 'F#': 6, 'G': 7, 'G#': 8, 'A': 9, 'A#': 10, 'B': 11}\n",
        "  data['Feature 13'] = data['Feature 13'].map(pitch_mapping)\n",
        "  data['Feature 13'] = data['Feature 13'] * (np.pi / 6)\n",
        "  data['Feature 13'] = np.sin(data['Feature 13'])\n",
        "\n",
        "  return data\n",
        "\n",
        "def feature_interaction(data: pd.DataFrame, interaction_features):\n",
        "  interation = pd.DataFrame()\n",
        "\n",
        "  for feature_pair in interaction_features:\n",
        "    feature1, feature2, new_feature_name = feature_pair\n",
        "    interation[new_feature_name] = data[feature1] * data[feature2]\n",
        "\n",
        "  tar_pos = data.columns.get_loc('Feature 13')\n",
        "\n",
        "  for new_feature_name in interation.columns:\n",
        "    tar_pos += 1\n",
        "    data.insert(loc=tar_pos, column=new_feature_name, value=interation[new_feature_name])\n",
        "\n",
        "  return data\n",
        "\n",
        "def normalize(data: pd.DataFrame, target: list):\n",
        "  data_copy = data.copy()  # Create a copy of the DataFrame to avoid modifying it in place\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  for tar in target:\n",
        "      data_copy[tar] = scaler.fit_transform(data_copy[[tar]])\n",
        "\n",
        "  return data_copy\n",
        "\n",
        "def standardize(data: pd.DataFrame, target: list):\n",
        "  data_copy = data.copy()  # Create a copy of the DataFrame to avoid modifying it in place\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  for tar in target:\n",
        "      data_copy[tar] = scaler.fit_transform(data_copy[[tar]])\n",
        "\n",
        "  return data_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRPAo7VvIMH"
      },
      "source": [
        "### Load Data & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HYpR73eS0ec"
      },
      "outputs": [],
      "source": [
        "normalize_target = [\n",
        "    'Feature 4', 'Feature 5', 'Feature 8'\n",
        "]\n",
        "\n",
        "standardize_target = [\n",
        "    'Feature 9', 'Feature 13'\n",
        "]\n",
        "\n",
        "interaction = [\n",
        "    ('Feature 10', 'Feature 12', 'vocal_instrument')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKLj5M2Qulmj"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(config['train_data'])\n",
        "test_data = pd.read_csv(config['test_data'])\n",
        "\n",
        "song_id = train_data['song_id']\n",
        "train_data.drop(['song_id'], axis = 1, inplace= True)\n",
        "\n",
        "train_data = pitch_processing(train_data)\n",
        "train_data = feature_interaction(train_data, interaction)\n",
        "\n",
        "feature_names = train_data.columns.to_list()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "\n",
        "processed_data = pd.DataFrame(train_data, columns=feature_names)\n",
        "processed_data.insert(loc = 0, column = 'song_id',value = song_id)\n",
        "processed_data.to_csv(\"processed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMOsKry10MFS"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUmKX_QY0HhE"
      },
      "source": [
        "### KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dCdjqa8sp-K"
      },
      "outputs": [],
      "source": [
        "costs = []\n",
        "for k in range(1, 30):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    kmeans.fit(train_data)\n",
        "    costs.append(kmeans.inertia_)  # inertia_ 是 KMeans 模型的成本函數值\n",
        "\n",
        "\n",
        "plt.plot(range(1, 30), costs, marker='o')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Cost (Inertia)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw2_IfS-vQiE"
      },
      "outputs": [],
      "source": [
        "k_values = range(5, 15)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(train_data)\n",
        "    silhouette_avg = silhouette_score(train_data, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Analysis for Optimal k')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgVNChz30akn"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=9, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(train_data)\n",
        "\n",
        "distances = kmeans.transform(train_data)\n",
        "\n",
        "min_values = np.min(distances, axis=1, keepdims=True)\n",
        "\n",
        "distances = np.where(distances == min_values, np.inf, distances)\n",
        "\n",
        "sc_val = min_values / distances\n",
        "\n",
        "print(sc_val)\n",
        "\n",
        "has_value_above_threshold = np.any(sc_val > config['threshold'], axis=1)\n",
        "indices_with_value_above_threshold = np.where(has_value_above_threshold)[0]\n",
        "\n",
        "wrong_ratio = len(indices_with_value_above_threshold) / len(sc_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHfVMdjR0LgI"
      },
      "source": [
        "### DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qlasOl10KqF",
        "outputId": "f65eb947-b17e-45e5-915b-17b37f888c48"
      },
      "outputs": [],
      "source": [
        "best_score = -1\n",
        "best_params = {'eps': None, 'min_samples': None}\n",
        "eps_val = [1.0, 1.5, 2.0]\n",
        "min_samples_val = [5, 10, 15, 20]\n",
        "\n",
        "for eps in eps_val:\n",
        "    for min_samples in min_samples_val:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(train_data)\n",
        "        silhouette = silhouette_score(train_data, labels)\n",
        "\n",
        "        if silhouette > best_score:\n",
        "            best_score = silhouette\n",
        "            best_params['eps'] = eps\n",
        "            best_params['min_samples'] = min_samples\n",
        "\n",
        "print(\"Best Parameters:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kfviD_Vq12V"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=2, min_samples=10)\n",
        "cluster_labels = dbscan.fit_predict(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVeVeTaf0U3i"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaecQ9MQ2PwT"
      },
      "outputs": [],
      "source": [
        "data_with_labels = np.column_stack((song_id, cluster_labels))\n",
        "\n",
        "result = pd.DataFrame(data_with_labels, columns=['song_id', 'Cluster_Label'])\n",
        "\n",
        "result.to_csv('cluster_labels.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "PSOTqko7Sf41",
        "outputId": "23884479-693c-42e5-c21b-7e6840482a29"
      },
      "outputs": [],
      "source": [
        "result_counts = result['Cluster_Label'].value_counts()\n",
        "print(result_counts)\n",
        "plt.bar(result_counts.index, result_counts.values)\n",
        "plt.xlabel('Clustering')\n",
        "plt.ylabel('Number')\n",
        "plt.title('Clustering Result')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fJoLQ2h2jVD"
      },
      "outputs": [],
      "source": [
        "song1 = test_data['col_1']\n",
        "song2 = test_data['col_2']\n",
        "\n",
        "test_len = len(song1)\n",
        "\n",
        "res = []\n",
        "\n",
        "for i in range(test_len):\n",
        "  song1_id = song1[i]\n",
        "  song2_id = song2[i]\n",
        "  if cluster_labels[song1_id] != cluster_labels[song2_id] :\n",
        "    res.append(0)\n",
        "  else:\n",
        "    res.append(1)\n",
        "\n",
        "id = [str(i) for i in range(len(res))]\n",
        "\n",
        "result = pd.DataFrame(res, index=id, columns=['ans'])\n",
        "result.index.name = 'id'\n",
        "\n",
        "result.to_csv('result.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fHfVMdjR0LgI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
